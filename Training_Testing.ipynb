{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25370f7d-a0f4-4fd5-b6f5-6db8f1f674da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### smote enn, rf, balance rf, xgboost, LR; \n",
    "\n",
    "##### cross val in train, test in different data\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(X):\n",
    "    numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_numeric_imputed = pd.DataFrame(imputer.fit_transform(X[numeric_cols]), columns=numeric_cols, index=X.index)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_numeric_scaled = pd.DataFrame(scaler.fit_transform(X_numeric_imputed), columns=numeric_cols, index=X.index)\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        X_categorical = X[categorical_cols].apply(LabelEncoder().fit_transform)\n",
    "    else:\n",
    "        X_categorical = pd.DataFrame(index=X.index)\n",
    "\n",
    "    X_processed = pd.concat([X_numeric_scaled, X_categorical], axis=1)\n",
    "    return X_processed\n",
    "\n",
    "# Train and evaluate model with cross-validation\n",
    "def train_with_cross_validation(model, model_name, train_data, feature_columns, target_column):\n",
    "    X_train = preprocess_data(train_data[feature_columns])\n",
    "    y_train = train_data[target_column]\n",
    "\n",
    "    # Encode target variable if using XGBoost\n",
    "    if model_name == \"XGBoost\":\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "    # Apply SMOTEENN\n",
    "    smote_enn = SMOTEENN(random_state=42)\n",
    "    X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Perform cross-validation on resampled train data\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_resampled, y_resampled, cv=skf, scoring='accuracy')\n",
    "\n",
    "    print(f\"\\n{model_name} Cross-validation results on training data:\")\n",
    "    for i, score in enumerate(cv_scores):\n",
    "        print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(cv_scores):.4f}\")\n",
    "    print(f\"Standard Deviation: {np.std(cv_scores):.4f}\")\n",
    "\n",
    "    # Train final model on full resampled training set\n",
    "    model.fit(X_resampled, y_resampled)\n",
    "    model_path = f\"{model_name}_final_model.joblib\"\n",
    "    dump(model, model_path)\n",
    "    print(f\"Final {model_name} model saved at: {model_path}\")\n",
    "\n",
    "# Test model on separate test set\n",
    "def test_model(test_data, feature_columns, target_column, model_name):\n",
    "    # Load the model\n",
    "    model_path = f\"{model_name}_final_model.joblib\"\n",
    "    clf = load(model_path)\n",
    "\n",
    "    # Prepare test data\n",
    "    X_test = preprocess_data(test_data[feature_columns])\n",
    "    y_test = test_data[target_column]\n",
    "\n",
    "    # Encode target variable for XGBoost\n",
    "    if model_name == \"XGBoost\":\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_test = label_encoder.fit_transform(y_test)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\n{model_name} Test Set Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    feature_columns = [\n",
    "        'maintainability_index', 'complexity_score', 'effort', 'difficulty', 'bugs',\n",
    "        'vocabulary', 'volume', 'multi', 'length', 'comments', 'calculated_length', 'time', 'blank'\n",
    "    ]\n",
    "    target_column = 'Risk_Group'\n",
    "\n",
    "    # Load train and test datasets\n",
    "    train_file_path = \"/../../training.csv\"\n",
    "    test_file_path = \"/../../testing.csv\"\n",
    "\n",
    "    train_data = pd.read_csv(train_file_path)\n",
    "    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "    # Remove duplicates in train data if necessary\n",
    "    train_data = train_data.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Models\n",
    "    models = {\n",
    "        \"BalancedRandomForest\": BalancedRandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='log2',\n",
    "            bootstrap=False,\n",
    "            criterion='gini'\n",
    "        ),\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42\n",
    "        ),\n",
    "        \"RandomForest\": RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='log2',\n",
    "            bootstrap=False,\n",
    "            criterion='gini'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Train and test each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name} with cross-validation...\")\n",
    "        train_with_cross_validation(model, model_name, train_data, feature_columns, target_column)\n",
    "        \n",
    "        print(f\"\\nTesting {model_name} on separate test set...\")\n",
    "        test_model(test_data, feature_columns, target_column, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a548d97f-779d-4c8d-b6d5-38737fa0b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "### no smote enn, rf, balance rf, xgboost, LR; \n",
    "### cross val in train, test in different data\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(X):\n",
    "    numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_numeric_imputed = pd.DataFrame(imputer.fit_transform(X[numeric_cols]), columns=numeric_cols, index=X.index)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_numeric_scaled = pd.DataFrame(scaler.fit_transform(X_numeric_imputed), columns=numeric_cols, index=X.index)\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        X_categorical = X[categorical_cols].apply(LabelEncoder().fit_transform)\n",
    "    else:\n",
    "        X_categorical = pd.DataFrame(index=X.index)\n",
    "\n",
    "    X_processed = pd.concat([X_numeric_scaled, X_categorical], axis=1)\n",
    "    return X_processed\n",
    "\n",
    "# Train and evaluate model with cross-validation\n",
    "def train_with_cross_validation(model, model_name, train_data, feature_columns, target_column):\n",
    "    X_train = preprocess_data(train_data[feature_columns])\n",
    "    y_train = train_data[target_column]\n",
    "\n",
    "    # Encode target variable if using XGBoost\n",
    "    if model_name == \"XGBoost\":\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "    # Perform cross-validation on train data only\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='accuracy')\n",
    "\n",
    "    print(f\"\\n{model_name} Cross-validation results on training data:\")\n",
    "    for i, score in enumerate(cv_scores):\n",
    "        print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(cv_scores):.4f}\")\n",
    "    print(f\"Standard Deviation: {np.std(cv_scores):.4f}\")\n",
    "\n",
    "    # Train final model on full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    model_path = f\"{model_name}_final_model.joblib\"\n",
    "    dump(model, model_path)\n",
    "    print(f\"Final {model_name} model saved at: {model_path}\")\n",
    "\n",
    "# Test model on separate test set\n",
    "def test_model(test_data, feature_columns, target_column, model_name):\n",
    "    # Load the model\n",
    "    model_path = f\"{model_name}_final_model.joblib\"\n",
    "    clf = load(model_path)\n",
    "\n",
    "    # Prepare test data\n",
    "    X_test = preprocess_data(test_data[feature_columns])\n",
    "    y_test = test_data[target_column]\n",
    "\n",
    "    # Encode target variable for XGBoost\n",
    "    if model_name == \"XGBoost\":\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_test = label_encoder.fit_transform(y_test)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\n{model_name} Test Set Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    feature_columns = [\n",
    "        'maintainability_index', 'complexity_score', 'effort', 'difficulty', 'bugs',\n",
    "        'vocabulary', 'volume', 'multi', 'length', 'comments', 'calculated_length', 'time', 'blank'\n",
    "    ]\n",
    "    target_column = 'Risk_Group'\n",
    "\n",
    "    # Load train and test datasets\n",
    "    train_file_path = \"/../../training.csv\"\n",
    "    test_file_path = \"/../../testing.csv\"\n",
    "\n",
    "    train_data = pd.read_csv(train_file_path)\n",
    "    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "    # Remove duplicates in train data if necessary\n",
    "    train_data = train_data.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Models\n",
    "    models = {\n",
    "        \"BalancedRandomForest\": BalancedRandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='log2',\n",
    "            bootstrap=False,\n",
    "            criterion='gini'\n",
    "        ),\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42\n",
    "        ),\n",
    "        \"RandomForest\": RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='log2',\n",
    "            bootstrap=False,\n",
    "            criterion='gini'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Train and test each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name} with cross-validation...\")\n",
    "        train_with_cross_validation(model, model_name, train_data, feature_columns, target_column)\n",
    "        \n",
    "        print(f\"\\nTesting {model_name} on separate test set...\")\n",
    "        test_model(test_data, feature_columns, target_column, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b74a6d-f3c5-4893-abfb-08b10075411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### no smote enn, rf, balance rf, xgboost, LR;  \n",
    "#cross val, train, test in different data\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(X):\n",
    "    numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_numeric_imputed = pd.DataFrame(imputer.fit_transform(X[numeric_cols]), columns=numeric_cols, index=X.index)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_numeric_scaled = pd.DataFrame(scaler.fit_transform(X_numeric_imputed), columns=numeric_cols, index=X.index)\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        X_categorical = X[categorical_cols].apply(LabelEncoder().fit_transform)\n",
    "    else:\n",
    "        X_categorical = pd.DataFrame(index=X.index)\n",
    "\n",
    "    X_processed = pd.concat([X_numeric_scaled, X_categorical], axis=1)\n",
    "    return X_processed\n",
    "\n",
    "# Train and save model\n",
    "def train_model(model, model_name, train_data, feature_columns, target_column):\n",
    "    X_train = preprocess_data(train_data[feature_columns])\n",
    "    y_train = train_data[target_column]\n",
    "\n",
    "    # Encode target variable for XGBoost\n",
    "    if model_name == \"XGBoost\":\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "    # Train the model on full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    model_path = f\"{model_name}_final_model.joblib\"\n",
    "    dump(model, model_path)\n",
    "    print(f\"Final {model_name} model saved at: {model_path}\")\n",
    "\n",
    "# Test model on separate test set\n",
    "def test_model(test_data, feature_columns, target_column, model_name):\n",
    "    # Load the model\n",
    "    model_path = f\"{model_name}_final_model.joblib\"\n",
    "    clf = load(model_path)\n",
    "\n",
    "    # Prepare test data\n",
    "    X_test = preprocess_data(test_data[feature_columns])\n",
    "    y_test = test_data[target_column]\n",
    "\n",
    "    # Encode target variable for XGBoost\n",
    "    if model_name == \"XGBoost\":\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_test = label_encoder.fit_transform(y_test)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\n{model_name} Test Set Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    feature_columns = [\n",
    "        'maintainability_index', 'complexity_score', 'effort', 'difficulty', 'bugs',\n",
    "        'vocabulary', 'volume', 'multi', 'length', 'comments', 'calculated_length', 'time', 'blank'\n",
    "    ]\n",
    "    target_column = 'Risk_Group'\n",
    "\n",
    "    # Load train and test datasets\n",
    "    train_file_path = \"/../../training.csv\"\n",
    "    test_file_path = \"/../../testing.csv\"\n",
    "\n",
    "    train_data = pd.read_csv(train_file_path)\n",
    "    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "    # Remove duplicates in train data if necessary\n",
    "    train_data = train_data.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Models\n",
    "    models = {\n",
    "        \"BalancedRandomForest\": BalancedRandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='log2',\n",
    "            bootstrap=False,\n",
    "            criterion='gini'\n",
    "        ),\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42\n",
    "        ),\n",
    "        \"RandomForest\": RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='log2',\n",
    "            bootstrap=False,\n",
    "            criterion='gini'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Train and test each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        train_model(model, model_name, train_data, feature_columns, target_column)\n",
    "        \n",
    "        print(f\"\\nTesting {model_name} on separate test set...\")\n",
    "        test_model(test_data, feature_columns, target_column, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749dd19-298b-437b-bf10-1c1fcb4d21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### smote enn, rf, balance rf, xgboost, LR;  \n",
    "### cross val, train, test in different data\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(X):\n",
    "    numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_numeric_imputed = pd.DataFrame(imputer.fit_transform(X[numeric_cols]), columns=numeric_cols, index=X.index)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_numeric_scaled = pd.DataFrame(scaler.fit_transform(X_numeric_imputed), columns=numeric_cols, index=X.index)\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        X_categorical = X[categorical_cols].apply(LabelEncoder().fit_transform)\n",
    "    else:\n",
    "        X_categorical = pd.DataFrame(index=X.index)\n",
    "\n",
    "    X_processed = pd.concat([X_numeric_scaled, X_categorical], axis=1)\n",
    "    return X_processed\n",
    "\n",
    "# Train model with SMOTE-ENN and save it\n",
    "def train_model_with_smoteenn(model, model_name, train_data, feature_columns, target_column):\n",
    "    X_train = preprocess_data(train_data[feature_columns])\n",
    "    y_train = train_data[target_column]\n",
    "\n",
    "    # Apply SMOTE-ENN for resampling\n",
    "    smote_enn = SMOTEENN(random_state=42)\n",
    "    X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Encode target variable for XGBoost\n",
    "    if model_name == \"XGBoost\":\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_resampled = label_encoder.fit_transform(y_resampled)\n",
    "\n",
    "    # Train the model on resampled data\n",
    "    model.fit(X_resampled, y_resampled)\n",
    "    model_path = f\"{model_name}_final_model_with_smoteenn.joblib\"\n",
    "    dump(model, model_path)\n",
    "    print(f\"Final {model_name} model with SMOTE-ENN saved at: {model_path}\")\n",
    "\n",
    "# Test model on separate test set\n",
    "def test_model(test_data, feature_columns, target_column, model_name):\n",
    "    # Load the model\n",
    "    model_path = f\"{model_name}_final_model_with_smoteenn.joblib\"\n",
    "    clf = load(model_path)\n",
    "\n",
    "    # Prepare test data\n",
    "    X_test = preprocess_data(test_data[feature_columns])\n",
    "    y_test = test_data[target_column]\n",
    "\n",
    "    # Encode target variable for XGBoost\n",
    "    if model_name == \"XGBoost\":\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_test = label_encoder.fit_transform(y_test)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\n{model_name} Test Set Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    feature_columns = [\n",
    "        'maintainability_index', 'complexity_score', 'effort', 'difficulty', 'bugs',\n",
    "        'vocabulary', 'volume', 'multi', 'length', 'comments', 'calculated_length', 'time', 'blank'\n",
    "    ]\n",
    "    target_column = 'Risk_Group'\n",
    "\n",
    "    # Load train and test datasets\n",
    "    train_file_path = \"/../../training.csv\"\n",
    "    test_file_path = \"/../../testing.csv\"\n",
    "\n",
    "    train_data = pd.read_csv(train_file_path)\n",
    "    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "    # Remove duplicates in train data if necessary\n",
    "    train_data = train_data.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Models\n",
    "    models = {\n",
    "        \"BalancedRandomForest\": BalancedRandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='log2',\n",
    "            bootstrap=False,\n",
    "            criterion='gini'\n",
    "        ),\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42\n",
    "        ),\n",
    "        \"RandomForest\": RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='log2',\n",
    "            bootstrap=False,\n",
    "            criterion='gini'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Train and test each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name} with SMOTE-ENN...\")\n",
    "        train_model_with_smoteenn(model, model_name, train_data, feature_columns, target_column)\n",
    "        \n",
    "        print(f\"\\nTesting {model_name} on separate test set...\")\n",
    "        test_model(test_data, feature_columns, target_column, model_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
