{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "477beec5-eb6d-4931-b212-cc6049990f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  /usr/local/anaconda3/bin/python -m pip uninstall [options] <package> ...\n",
      "  /usr/local/anaconda3/bin/python -m pip uninstall [options] -r <requirements file> ...\n",
      "\n",
      "no such option: -Y\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall openpyxl -Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c5dafe-96c5-43ca-8670-f4e2e76978cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in /../../.local/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/anaconda3/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad1e88b-6fad-46f8-8c03-a06ab29e01ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in /../../.local/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /../../anaconda3/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca4aba4-159c-474c-98f1-bb8f4d9672c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /../../.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /../../.local/lib/python3.10/site-packages (from pandas) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /../../.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /../../.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f5a881-49a8-42c0-a77f-4f94c6d143a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /../../.local/lib/python3.10/site-packages (2.1.0)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Downloading numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.0\n",
      "    Uninstalling numpy-2.1.0:\n",
      "      Successfully uninstalled numpy-2.1.0\n",
      "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/../../.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/../../.local/lib/python3.10/site-packages/~umpy.libs'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/../../.local/lib/python3.10/site-packages/~umpy'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 2.1.1 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ea8fa8-8c2d-4643-bca9-49194e4bf56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for Sheet1:\n",
      "TTI_maintainability_index    1.722146\n",
      "TTI_complexity_score         2.264662\n",
      "TTI_effort                   4.220200\n",
      "TTI_difficulty               1.293426\n",
      "TTI_bugs                     2.194645\n",
      "TTI_vocabulary               1.769783\n",
      "TTI_volume                   2.194645\n",
      "dtype: float64\n",
      "Averages for Sheet2:\n",
      "TTI_maintainability_index    1.030651\n",
      "TTI_complexity_score         1.883615\n",
      "TTI_effort                   3.137207\n",
      "TTI_difficulty               1.557891\n",
      "TTI_bugs                     2.094004\n",
      "TTI_vocabulary               1.792366\n",
      "TTI_volume                   2.094004\n",
      "dtype: float64\n",
      "Processed and saved Excel file with multiple sheets to /../../filtered_technicalDebt.xlsx\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "################################################technical debt\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "####################### Step 1: Clean the Data\n",
    "\n",
    "# Load the Excel file (with multiple sheets)\n",
    "file_path = '/../../merged_file.xlsx'\n",
    "\n",
    "# Read all sheets into a dictionary of DataFrames\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Specify the columns to keep for each sheet\n",
    "columns_to_keep = ['file', 'module_name', 'complexity_rank', 'maintainability_index',\n",
    "                   'complexity_score', 'effort', 'difficulty', 'bugs', 'vocabulary', 'volume']\n",
    "\n",
    "# Filter the data for each sheet\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    # Keep only the specified columns\n",
    "    filtered_df = df[columns_to_keep]\n",
    "    \n",
    "    # Drop columns that contain any null values\n",
    "    sheets_dict[sheet_name] = filtered_df.dropna(axis=1, how='any')\n",
    "\n",
    "####################### Step 2: Perform TTI and Ti Calculations\n",
    "\n",
    "# Adjusting the function to correctly apply the thresholds element-wise\n",
    "def calculate_metric_cost(row, lt, ut):\n",
    "    metric_costs = {}\n",
    "    for column in row.index:\n",
    "        value = row[column]\n",
    "        lt_value = lt[column]\n",
    "        ut_value = ut[column]\n",
    "        if value < lt_value:\n",
    "            metric_costs[column] = value / lt_value\n",
    "        elif value > ut_value:\n",
    "            metric_costs[column] = value / ut_value\n",
    "        else:\n",
    "            metric_costs[column] = value / min(lt_value, ut_value)\n",
    "    return pd.Series(metric_costs)\n",
    "\n",
    "# Process each sheet for TTI and Ti calculations\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    metric_columns = df.columns[2:]  # Assuming the first two columns are 'file' and 'module_name'\n",
    "    \n",
    "    LT = df[metric_columns].quantile(0.25)\n",
    "    UT = df[metric_columns].quantile(0.75)\n",
    "\n",
    "    # Apply the function across each row in the dataframe\n",
    "    tti_df = df[metric_columns].apply(calculate_metric_cost, axis=1, lt=LT, ut=UT)\n",
    "\n",
    "    # Add the TTI results back to the original dataframe, prefixed with 'TTI_'\n",
    "    for col in tti_df.columns:\n",
    "        df[f'TTI_{col}'] = tti_df[col]\n",
    "\n",
    "    # Calculate the average TTI and add it as a new column\n",
    "    df['TTI_adjusted'] = tti_df.sum(axis=1) / (4 * tti_df.shape[1])\n",
    "\n",
    "  ############ Ti Calculation\n",
    "\n",
    "    TTI_volume_index = df.columns.get_loc('TTI_volume')\n",
    "\n",
    "# Updated list of columns to consider for the operation\n",
    "    columns_to_consider = df.columns[9: TTI_volume_index + 1]\n",
    "    \n",
    "    # Calculate the average of each column\n",
    "    averages = df[columns_to_consider].mean()\n",
    "    print(f\"Averages for {sheet_name}:\")\n",
    "    print(averages)\n",
    "\n",
    "    # Apply the condition: if the value is equal to or above average, set Ti=1, else Ti=0\n",
    "    for col in columns_to_consider:\n",
    "        df[f'Ti_{col}'] = df[col].apply(lambda x: 1 if x >= averages[col] else 0)\n",
    "    \n",
    "    # Calculate the sum of all Ti_ columns and then find the average\n",
    "    ti_columns = [col for col in df.columns if col.startswith('Ti_')]\n",
    "    df['Ti_adjusted'] = df[ti_columns].sum(axis=1) / (4 * len(ti_columns))\n",
    "\n",
    "    # Add the two columns TTI_adjusted and Ti_adjusted\n",
    "    df['Total_T_adjusted'] = df['TTI_adjusted'] + df['Ti_adjusted']\n",
    "\n",
    "####################### Step 3: Save the DataFrame and Color the Headers\n",
    "\n",
    "# Save the updated dataframes to a new Excel file (with multiple sheets)\n",
    "output_path_with_ti_average = '/../../filtered_technicalDebt.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_path_with_ti_average, engine='openpyxl') as writer:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Load the workbook and apply styling to each sheet\n",
    "wb = load_workbook(output_path_with_ti_average)\n",
    "\n",
    "# Define the green, red, and blue fills\n",
    "green_fill = PatternFill(start_color=\"00FF00\", end_color=\"00FF00\", fill_type=\"solid\")\n",
    "red_fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")\n",
    "blue_fill = PatternFill(start_color=\"0000FF\", end_color=\"0000FF\", fill_type=\"solid\")\n",
    "\n",
    "# Apply the coloring logic to each sheet\n",
    "for sheet_name in wb.sheetnames:\n",
    "    ws = wb[sheet_name]\n",
    "    \n",
    "    # Apply the green fill to the titles from 'file' to 'volume'\n",
    "    for cell in ws[1]:  # First row contains the headers\n",
    "        if cell.value in ['file', 'module_name', 'complexity_rank', 'maintainability_index',\n",
    "                          'complexity_score', 'effort', 'difficulty', 'bugs', 'vocabulary', 'volume']:\n",
    "            cell.fill = green_fill\n",
    "    \n",
    "    # Apply the red fill to TTI_ columns' titles and blue fill to Ti_ columns' titles\n",
    "    for cell in ws[1]:  # First row contains the headers\n",
    "        if cell.value.startswith('TTI_'):\n",
    "            cell.fill = red_fill\n",
    "        elif cell.value.startswith('Ti_'):\n",
    "            cell.fill = blue_fill\n",
    "\n",
    "# Save the workbook with the updated styles\n",
    "wb.save(output_path_with_ti_average)\n",
    "\n",
    "print(f\"Processed and saved Excel file with multiple sheets to {output_path_with_ti_average}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b70b9e85-d30f-4f48-8f40-390b6032a9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for Sheet1:\n",
      "MTI_blank                1.821411\n",
      "MTI_comments             1.671049\n",
      "MTI_calculated_length    2.159159\n",
      "dtype: float64\n",
      "Averages for Sheet2:\n",
      "MTI_blank                1.483580\n",
      "MTI_comments             2.386457\n",
      "MTI_calculated_length    2.354788\n",
      "dtype: float64\n",
      "Processed and saved Excel file with multiple sheets to /../..//Desktop/../../filtered_Metrics.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "####################### Step 1: Clean the Data\n",
    "\n",
    "# Load the Excel file (with multiple sheets)\n",
    "file_path = '/../../merged_file.xlsx'\n",
    "\n",
    "# Read all sheets into a dictionary of DataFrames\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Specify the columns to keep for each sheet\n",
    "columns_to_keep = ['file', 'module_name', 'loc', 'category', 'lloc', 'sloc', 'multi', 'length', 'blank', 'comments', 'calculated_length']\n",
    "\n",
    "# Filter the data for each sheet\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    # Convert 'category' column to numeric values (if it exists in the sheet)\n",
    "    if 'category' in df.columns:\n",
    "        df['category_numeric'] = label_encoder.fit_transform(df['category'])\n",
    "    else:\n",
    "        df['category_numeric'] = None\n",
    "    \n",
    "    # Keep only the specified columns\n",
    "    filtered_df = df[columns_to_keep]\n",
    "    \n",
    "    # Convert relevant columns to numeric, coerce errors to NaN\n",
    "    filtered_df = filtered_df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Drop columns that contain any null values\n",
    "    filtered_df = filtered_df.dropna(axis=1, how='any')\n",
    "    \n",
    "    # Drop rows with NaNs in the remaining columns\n",
    "    filtered_df = filtered_df.dropna()\n",
    "    \n",
    "    sheets_dict[sheet_name] = filtered_df\n",
    "\n",
    "####################### Step 2: Perform MTI and Mi Calculations\n",
    "\n",
    "# Adjusting the function to correctly apply the thresholds element-wise\n",
    "def calculate_metric_cost(row, lt, ut):\n",
    "    metric_costs = {}\n",
    "    for column in row.index:\n",
    "        value = row[column]\n",
    "        lt_value = lt[column]\n",
    "        ut_value = ut[column]\n",
    "        \n",
    "        # Handle cases where lt_value or ut_value might be zero or NaN\n",
    "        if pd.isna(lt_value) or pd.isna(ut_value) or lt_value == 0 or ut_value == 0:\n",
    "            metric_costs[column] = 0  # Assign 0 or any other default value\n",
    "        else:\n",
    "            if value < lt_value:\n",
    "                metric_costs[column] = value / lt_value\n",
    "            elif value > ut_value:\n",
    "                metric_costs[column] = value / ut_value\n",
    "            else:\n",
    "                metric_costs[column] = value / min(lt_value, ut_value)\n",
    "    return pd.Series(metric_costs)\n",
    "\n",
    "# Process each sheet for MTI and Mi calculations\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    metric_columns = df.columns[2:]  # Assuming the first two columns are 'file' and 'module_name'\n",
    "    \n",
    "    LT = df[metric_columns].quantile(0.25)\n",
    "    UT = df[metric_columns].quantile(0.75)\n",
    "\n",
    "    # Apply the function across each row in the dataframe\n",
    "    mti_df = df[metric_columns].apply(calculate_metric_cost, axis=1, lt=LT, ut=UT)\n",
    "\n",
    "    # Add the MTI results back to the original dataframe, prefixed with 'MTI_'\n",
    "    for col in mti_df.columns:\n",
    "        df[f'MTI_{col}'] = mti_df[col]\n",
    "\n",
    "    # Calculate the average MTI and add it as a new column\n",
    "    df['MTI_adjusted'] = mti_df.sum(axis=1) / (4 * mti_df.shape[1])\n",
    "\n",
    "    ############ Mi Calculation\n",
    "\n",
    "    MTI_calculated_length_index = df.columns.get_loc('MTI_calculated_length')\n",
    "\n",
    "    # Updated list of columns to consider for the operation\n",
    "    columns_to_consider = df.columns[11: MTI_calculated_length_index + 1]\n",
    "    \n",
    "    # Calculate the average of each column\n",
    "    averages = df[columns_to_consider].mean()\n",
    "    print(f\"Averages for {sheet_name}:\")\n",
    "    print(averages)\n",
    "\n",
    "    # Apply the condition: if the value is equal to or above average, set Mi=1, else Mi=0\n",
    "    for col in columns_to_consider:\n",
    "        df[f'Mi_{col}'] = df[col].apply(lambda x: 1 if x >= averages[col] else 0)\n",
    "    \n",
    "    # Calculate the sum of all Mi_ columns and then find the average\n",
    "    mi_columns = [col for col in df.columns if col.startswith('Mi_')]\n",
    "    df['Mi_adjusted'] = df[mi_columns].sum(axis=1) / (4 * len(mi_columns))\n",
    "\n",
    "    # Add the two columns MTI_adjusted and Mi_adjusted\n",
    "    df['Total_M_adjusted'] = df['MTI_adjusted'] + df['Mi_adjusted']\n",
    "\n",
    "####################### Step 3: Save the DataFrame and Color the Headers\n",
    "\n",
    "# Save the updated dataframes to a new Excel file (with multiple sheets)\n",
    "output_path_with_mi_average = '/../../filtered_Metrics.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_path_with_mi_average, engine='openpyxl') as writer:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Load the workbook and apply styling to each sheet\n",
    "wb = load_workbook(output_path_with_mi_average)\n",
    "\n",
    "# Define the green, red, and blue fills\n",
    "green_fill = PatternFill(start_color=\"00FF00\", end_color=\"00FF00\", fill_type=\"solid\")\n",
    "red_fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")\n",
    "blue_fill = PatternFill(start_color=\"0000FF\", end_color=\"0000FF\", fill_type=\"solid\")\n",
    "\n",
    "# Apply the coloring logic to each sheet\n",
    "for sheet_name in wb.sheetnames:\n",
    "    ws = wb[sheet_name]\n",
    "    \n",
    "    # Apply the green fill to the titles from 'file' to 'calculated_length'\n",
    "    for cell in ws[1]:  # First row contains the headers\n",
    "        if cell.value in ['file', 'module_name', 'loc', 'category', 'lloc', 'sloc', 'multi', 'length', 'blank', 'comments', 'calculated_length']:\n",
    "            cell.fill = green_fill\n",
    "    \n",
    "    # Apply the red fill to MTI_ columns' titles and blue fill to Mi_ columns' titles\n",
    "    for cell in ws[1]:  # First row contains the headers\n",
    "        if cell.value.startswith('MTI_'):\n",
    "            cell.fill = red_fill\n",
    "        elif cell.value.startswith('Mi_'):\n",
    "            cell.fill = blue_fill\n",
    "\n",
    "# Save the workbook with the updated styles\n",
    "wb.save(output_path_with_mi_average)\n",
    "\n",
    "print(f\"Processed and saved Excel file with multiple sheets to {output_path_with_mi_average}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8857865b-4719-4bbd-be06-7cace92521a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved Sheet1 to /../../SF.xlsx\n",
      "Processed and saved Sheet2 to/../../SF.xlsx\n",
      "All sheets processed and files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#severity calculation\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "metrics_file_path = '/../../filtered_Metrics.xlsx'\n",
    "TD_file_path = '/../../filtered_technicalDebt.xlsx'\n",
    "output_combined_path = '/../../combined.xlsx'\n",
    "output_path_grouped = '/../../SF.xlsx'\n",
    "\n",
    "# Ensure the directories exist for saving the output files\n",
    "output_dir_combined = os.path.dirname(output_combined_path)\n",
    "output_dir_grouped = os.path.dirname(output_path_grouped)\n",
    "os.makedirs(output_dir_combined, exist_ok=True)\n",
    "os.makedirs(output_dir_grouped, exist_ok=True)\n",
    "\n",
    "# Load the necessary columns from the metrics file\n",
    "metrics_sheets = pd.read_excel(metrics_file_path, sheet_name=None)  # Load all sheets\n",
    "td_sheets = pd.read_excel(TD_file_path, sheet_name=None)  # Load all sheets\n",
    "\n",
    "# Define a function to process each sheet\n",
    "def process_sheet(sheet_name, df_metrics, df_td):\n",
    "    # Ensure the sheets have the same structure (index alignment)\n",
    "    df_combined = df_metrics.copy()\n",
    "    df_combined['Total_T_adjusted'] = df_td['Total_T_adjusted']\n",
    "    df_combined['Total_adjusted_combined'] = df_combined['Total_M_adjusted'] + df_combined['Total_T_adjusted']\n",
    "    \n",
    "    # Define the quartile thresholds\n",
    "    q1 = df_combined['Total_adjusted_combined'].quantile(0.25)\n",
    "    q2 = df_combined['Total_adjusted_combined'].quantile(0.50)\n",
    "    q3 = df_combined['Total_adjusted_combined'].quantile(0.75)\n",
    "    \n",
    "    # Function to assign groups based on quartile thresholds\n",
    "    def assign_group(value):\n",
    "        if value <= q1:\n",
    "            return 'Low'\n",
    "        elif q1 < value <= q2:\n",
    "            return 'Medium'\n",
    "        elif q2 < value <= q3:\n",
    "            return 'High'\n",
    "        else:\n",
    "            return 'Severe'\n",
    "    \n",
    "    # Apply the function to create a new column 'Risk_Group'\n",
    "    df_combined['Risk_Group'] = df_combined['Total_adjusted_combined'].apply(assign_group)\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "# Process each sheet\n",
    "for sheet_name, df_metrics in metrics_sheets.items():\n",
    "    if sheet_name in td_sheets:\n",
    "        df_td = td_sheets[sheet_name]\n",
    "        # Process and save each sheet\n",
    "        df_combined = process_sheet(sheet_name, df_metrics, df_td)\n",
    "        \n",
    "        # Save the updated DataFrame with the grouped data to a new Excel file\n",
    "        output_sheet_path = os.path.join(output_dir_grouped, f\"{sheet_name}_SF.xlsx\")\n",
    "        df_combined.to_excel(output_sheet_path, index=False)\n",
    "        print(f\"Processed and saved {sheet_name} to {output_sheet_path}\")\n",
    "\n",
    "print(\"All sheets processed and files saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44e12f9e-e5c4-45f3-bcec-ee6695b53b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####generating SF for training #CSV file generation for training # for now 2 CSV file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "sf_file_path = '/../../SF.xlsx'\n",
    "merged_file_path = '/../../merged_file.xlsx'\n",
    "output_risk_csv_path = '/../../training.csv'\n",
    "\n",
    "# Load the 'Risk_Group' column from the SF file\n",
    "df_sf = pd.read_excel(sf_file_path, usecols=['Risk_Group'])\n",
    "\n",
    "# Load the entire merged1 file\n",
    "df_merged = pd.read_excel(merged_file_path)\n",
    "\n",
    "# Add the 'Risk_Group' column to the merged DataFrame\n",
    "df_merged['Risk_Group'] = df_sf['Risk_Group']\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_merged.to_csv(output_risk_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59127fee-8955-4fe1-b1a9-eec1aeb60c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
